<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Emotion Song Player with Vinyl</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f0f0f0;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            margin: 0;
        }
        .container {
            display: flex;
            gap: 20px;
            background-color: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }
        .video-container {
            width: 300px;
            height: 300px;
            border: 2px dashed red;
            overflow: hidden;
        }
        .animation-container {
            width: 300px;
            height: 300px;
        }
        .emotion-container {
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            padding-left: 20px;
        }
        h2 {
            margin: 0;
            font-size: 20px;
        }
        #emotion {
            background-color: yellow;
            padding: 5px;
            border-radius: 5px;
        }
        #dial {
            margin-top: 20px;
        }
        audio {
            margin-top: 10px;
            display: none; /* Hide audio controls since we’re controlling playback */
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- Camera on the left -->
        <div class="video-container">
            <video id="video" width="300" height="300" autoplay></video>
        </div>
        <!-- Vinyl animation on the right -->
        <div class="animation-container" id="vinyl-animation"></div>
        <div class="emotion-container">
            <h2>Detected Emotion: <span id="emotion">Waiting...</span></h2>
            <canvas id="dial" width="200" height="200"></canvas>
            <audio id="audio" controls></audio>
        </div>
    </div>

    <!-- Load required libraries -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.9.0"></script>
    <script src="https://cdn.jsdelivr.net/npm/@vladmandic/face-api/dist/face-api.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/bodymovin/5.7.13/lottie.min.js"></script>
    <script>
        const video = document.getElementById('video');
        const audio = document.getElementById('audio');
        const emotionDisplay = document.getElementById('emotion');
        const canvas = document.getElementById('dial');
        const ctx = canvas.getContext('2d');
        let angle = 0;
        let detectionInterval;
        let playbackTimeout;
        let expressionCounts = { happy: 0, sad: 0, neutral: 0, angry: 0 };

        // Map emotions to online music URLs (replace these with real links)
        const songMap = {
            'happy': 'https://www.soundhelix.com/examples/mp3/SoundHelix-Song-1.mp3',
            'sad': 'https://www.soundhelix.com/examples/mp3/SoundHelix-Song-2.mp3',
            'neutral': 'https://www.soundhelix.com/examples/mp3/SoundHelix-Song-3.mp3',
            'angry': 'https://www.soundhelix.com/examples/mp3/SoundHelix-Song-4.mp3'
        };

        // Access webcam
        navigator.mediaDevices.getUserMedia({ video: true })
            .then(stream => {
                video.srcObject = stream;
            })
            .catch(error => {
                console.error('Error accessing webcam:', error);
                alert('Please allow webcam access, bro!');
            });

        // Load face-api.js models
        async function loadModel() {
            await faceapi.nets.tinyFaceDetector.loadFromUri('https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model/');
            await faceapi.nets.faceLandmark68Net.loadFromUri('https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model/');
            await faceapi.nets.faceExpressionNet.loadFromUri('https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model/');
        }

        loadModel().then(() => {
            console.log('Models loaded, bro!');
            startDetection();
        }).catch(error => {
            console.error('Error loading models:', error);
            alert('Trouble loading the face detection models, bro!');
        });

        // Start detection phase (5 seconds)
        function startDetection() {
            expressionCounts = { happy: 0, sad: 0, neutral: 0, angry: 0 };
            let detectionCount = 0;
            emotionDisplay.innerText = 'Detecting...';
            detectionInterval = setInterval(async () => {
                const detections = await faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions())
                    .withFaceLandmarks()
                    .withFaceExpressions();
                if (detections && detections.expressions) {
                    const emotion = detections.expressions.asSortedArray()[0].expression;
                    expressionCounts[emotion]++;
                } else {
                    // No "None" - just a helpful message
                    emotionDisplay.innerText = 'Yo, put your face in front of the cam!';
                }
                detectionCount++;
                if (detectionCount >= 5) {
                    clearInterval(detectionInterval);
                    const dominantEmotion = getDominantEmotion();
                    if (dominantEmotion) {
                        playSong(dominantEmotion);
                        emotionDisplay.innerText = dominantEmotion;
                        playbackTimeout = setTimeout(startDetection, 10000); // 10-second playback
                    } else {
                        emotionDisplay.innerText = 'Couldn’t figure out your vibe, bro!';
                        setTimeout(startDetection, 1000); // Restart quickly if no emotion
                    }
                }
            }, 1000); // Detect every second for 5 seconds
        }

        // Find the dominant emotion
        function getDominantEmotion() {
            let maxCount = 0;
            let dominantEmotion = null;
            for (const [emotion, count] of Object.entries(expressionCounts)) {
                if (count > maxCount) {
                    maxCount = count;
                    dominantEmotion = emotion;
                }
            }
            return dominantEmotion;
        }

        // Play song for the detected emotion
        function playSong(emotion) {
            if (songMap[emotion]) {
                audio.src = songMap[emotion];
                audio.play();
                setTimeout(() => audio.pause(), 10000); // Stop after 10 seconds
            } else {
                console.error('No song for this emotion, bro:', emotion);
            }
        }

        // Draw spinning dial
        function drawDial() {
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            ctx.save();
            ctx.translate(100, 100);
            ctx.rotate(angle);
            ctx.beginPath();
            ctx.arc(0, 0, 90, 0, 2 * Math.PI);
            ctx.stroke();
            ctx.beginPath();
            ctx.moveTo(0, 0);
            ctx.lineTo(90, 0);
            ctx.stroke();
            ctx.restore();
            angle += 0.01;
            requestAnimationFrame(drawDial);
        }
        drawDial();

        // Load vinyl animation on the right
        var animation = lottie.loadAnimation({
            container: document.getElementById('vinyl-animation'),
            renderer: 'svg',
            loop: true,
            autoplay: true,
            path: 'https://assets10.lottiefiles.com/packages/lf20_knpksg.json' // Vinyl animation from LottieFiles
        });
    </script>
</body>
</html>
